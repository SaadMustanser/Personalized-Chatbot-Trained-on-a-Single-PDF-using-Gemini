# -*- coding: utf-8 -*-
"""PDF Specified Chatbot (Gemini API).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Yp6DrV7MTiyDUK-KNyXeqM3U37ZgTRdq
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install -q -U google-generativeai
!pip install -q PyMuPDF

import fitz
import google.generativeai as genai
import textwrap
from IPython.display import display, Markdown
from google.colab import userdata

def extract_text_from_pdf(pdf_path):
    doc = fitz.open(pdf_path)
    text = ""
    for page in doc:
        text += page.get_text()
    return text

def to_markdown(text):
    text = text.replace('â€¢', '  *')
    return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))

# Used to securely store your API key
GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')
genai.configure(api_key=GOOGLE_API_KEY)

# Select the model
model_name = 'gemini-1.5-flash'
model = genai.GenerativeModel(model_name)

# Extract text from the PDF
pdf_path = "/content/drive/MyDrive/Saad Mustanser's Resume.pdf"
pdf_text = extract_text_from_pdf(pdf_path)

def get_gemini_response(prompt, context):
    try:
        response = model.generate_content(f"{context}\n\n{prompt}")
        return response.text
    except Exception as e:
        return f"Error: {str(e)}"

def chat(context):
    print("Chatbot: Hello! How can I help you today?")
    while True:
        user_input = input("You: ")
        if user_input.lower() == 'exit':
            print("Chatbot: Goodbye!")
            break
        # Generate response from the model using the PDF context
        response_text = get_gemini_response(user_input, context)
        display(to_markdown(response_text))

if __name__ == "__main__":
    chat(pdf_text)

